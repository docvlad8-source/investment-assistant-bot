import os
import logging
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from dotenv import load_dotenv
from openai import AsyncOpenAI
from rag_engine import retrieve_context, load_knowledge
from calculations import calculate_ytm, calculate_sharpe, calculate_pe

load_dotenv()
logging.basicConfig(level=logging.INFO)

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
load_knowledge()

client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üéì –ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ä—ã–Ω–∫—É —Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥.\n\n"
        "–°–ø—Ä–æ—Å–∏ –º–µ–Ω—è:\n"
        "‚Ä¢ –ß—Ç–æ —Ç–∞–∫–æ–µ –¥—é—Ä–∞—Ü–∏—è?\n"
        "‚Ä¢ –ö–∞–∫ —Å—á–∏—Ç–∞—Ç—å YTM?\n"
        "‚Ä¢ –û–±—ä—è—Å–Ω–∏ CAPM\n\n"
        "–ò–ª–∏ –Ω–∞—á–Ω–∏ —Ç–µ—Å—Ç: /test\n\n"
        "‚ö†Ô∏è –Ø –Ω–µ –¥–∞—é –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π!"
    )

async def handle_test(update: Update, context: ContextTypes.DEFAULT_TYPE):
    questions = [
        "–ß—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –±–µ—Ç–∞?",
        "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∫ –ø–æ–≥–∞—à–µ–Ω–∏—é (YTM) –æ–±–ª–∏–≥–∞—Ü–∏–∏?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å –ø–æ –ú–∞—Ä–∫–æ–≤–∏—Ü—É?"
    ]
    context.user_data['test_question'] = questions[0]
    await update.message.reply_text(f"üìù –í–æ–ø—Ä–æ—Å:\n{questions[0]}\n\n–ù–∞–ø–∏—à–∏—Ç–µ –≤–∞—à –æ—Ç–≤–µ—Ç:")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text.strip()
    
    # –†–µ–∂–∏–º —Ç–µ—Å—Ç–∞
    if 'test_question' in context.user_data:
        question = context.user_data['test_question']
        prompt = f"""
–û—Ü–µ–Ω–∏ –æ—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞ –ø–æ —à–∫–∞–ª–µ –æ—Ç 0 –¥–æ 100. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—É.

–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å:
- –î–ª—è –±–µ—Ç–∞: –º–µ—Ä—É —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä—ã–Ω–∫–∞.
- –ú–∞–∫—Å–∏–º—É–º 100 –±–∞–ª–ª–æ–≤ –∑–∞ –ø–æ–ª–Ω–æ—Ç—É, —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —è—Å–Ω–æ—Å—Ç—å.

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞: {text}

–û—Ü–µ–Ω–∫–∞ (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ):
"""
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        score = response.choices[0].message.content.strip()
        await update.message.reply_text(f"–í–∞—à–∞ –æ—Ü–µ–Ω–∫–∞: **{score}/100**\n–•–æ—Ç–∏—Ç–µ –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å? –ù–∞–ø–∏—à–∏—Ç–µ /test", parse_mode="Markdown")
        context.user_data.pop('test_question', None)
        return

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á
    if "ytm" in text.lower() or "–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∫ –ø–æ–≥–∞—à–µ–Ω–∏—é" in text.lower():
        # –ü—Ä–∏–º–µ—Ä: "–æ–±–ª–∏–≥–∞—Ü–∏—è 1000, –∫—É–ø–æ–Ω 8%, 3 –≥–æ–¥–∞, —Ü–µ–Ω–∞ 950"
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä—Å–µ—Ä ‚Äî –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
        explanation = (
            "–î–ª—è —Ä–∞—Å—á—ë—Ç–∞ YTM –Ω—É–∂–Ω—ã: –Ω–æ–º–∏–Ω–∞–ª, –∫—É–ø–æ–Ω, —Å—Ä–æ–∫, —Ü–µ–Ω–∞.\n"
            "–ü—Ä–∏–º–µ—Ä: –Ω–æ–º–∏–Ω–∞–ª=1000, –∫—É–ø–æ–Ω=80, —Å—Ä–æ–∫=3, —Ü–µ–Ω–∞=950 ‚Üí YTM ‚âà 9.87%.\n"
            "–•–æ—á–µ—à—å, —á—Ç–æ–±—ã —è –ø–æ—Å—á–∏—Ç–∞–ª –ø–æ —Ç–≤–æ–∏–º –¥–∞–Ω–Ω—ã–º? –ù–∞–ø–∏—à–∏: YTM –Ω–æ–º–∏–Ω–∞–ª=... –∫—É–ø–æ–Ω=... —Å—Ä–æ–∫=... —Ü–µ–Ω–∞=..."
        )
        await update.message.reply_text(explanation)
        return

    # –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí RAG + LLM
    context_chunks = retrieve_context(text)
    prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º –∏ —Ä—ã–Ω–∫—É —Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —á—ë—Ç–∫–æ, –ø–æ –¥–µ–ª—É.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ. –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏: "–ù–µ –∑–Ω–∞—é".

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —É—á–µ–±–Ω–∏–∫–æ–≤:
{context_chunks}

–í–æ–ø—Ä–æ—Å:
{text}

–û—Ç–≤–µ—Ç:
"""
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=500
        )
        answer = response.choices[0].message.content
        await update.message.reply_text(answer, parse_mode="Markdown")
    except Exception as e:
        logging.error(e)
        await update.message.reply_text("–ò–∑–≤–∏–Ω–∏, —Å–µ–π—á–∞—Å –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")

def main():
    app = Application.builder().token(os.getenv("TELEGRAM_BOT_TOKEN")).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("test", handle_test))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    app.run_polling()

if __name__ == "__main__":
    main()